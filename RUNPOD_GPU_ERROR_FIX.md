# üö® Troubleshooting: RunPod GPU Detection Error

## ‚ùå Erreur Rencontr√©e

```
error creating container: nvidia-smi: parsing output of line 0: 
failed to parse (pcie.link.gen.max) into int: 
strconv.Atoi: parsing "": invalid syntax
```

## üîç Cause du Probl√®me

Ce n'est **PAS** li√© au code de l'application. C'est RunPod qui essaie de d√©tecter les capacit√©s GPU avant de cr√©er le container et √©choue √† parser la sortie de `nvidia-smi`.

### Causes Possibles

1. **GPU Virtualis√© (vGPU)** : Certaines propri√©t√©s PCIe ne sont pas expos√©es
2. **Drivers NVIDIA incomplets** dans l'environnement RunPod
3. **Template RunPod incompatible** avec le GPU s√©lectionn√©
4. **Version nvidia-smi obsol√®te** ou incompatible

---

## ‚úÖ Solutions

### Solution 1: Workaround Dockerfile (D√©j√† Appliqu√©)

J'ai ajout√© ces variables d'environnement dans le Dockerfile :

```dockerfile
ENV NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    RUNPOD_SKIP_GPU_DETECTION=1
```

**Action** : Rebuild et redeploy l'image

```bash
# Build nouvelle image
docker build -t aleou/ffmpeg-worker:0.16.1-serverless --target final_serverless .

# Push vers registry
docker push aleou/ffmpeg-worker:0.16.1-serverless

# Update RunPod template avec nouvelle version
```

---

### Solution 2: Changer de GPU Template RunPod

Le probl√®me arrive souvent avec certains GPUs ou templates.

**Essayer dans cet ordre** :

1. **RTX 4090** (recommand√© pour inference)
2. **RTX A6000** 
3. **A40**
4. **RTX 3090**

**√Ä √©viter** :
- GPU virtualis√©s (vGPU)
- GPUs tr√®s anciens (< Pascal)
- Templates custom mal configur√©s

---

### Solution 3: Utiliser Base Image Diff√©rente

Si le probl√®me persiste, essayer avec une base image plus r√©cente :

```dockerfile
# Au lieu de:
FROM runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04

# Essayer:
FROM runpod/pytorch:2.4.0-py3.11-cuda12.4.0-devel-ubuntu22.04
# ou
FROM nvidia/cuda:12.4.0-cudnn-devel-ubuntu22.04
```

---

### Solution 4: Contacter Support RunPod

Si rien ne fonctionne, c'est probablement un bug RunPod.

**Informations √† fournir** :
- GPU Type s√©lectionn√©
- Template utilis√©
- Base image Docker
- Logs complets de l'erreur

---

## üß™ V√©rifications √† Faire

### 1. V√©rifier GPU Disponible sur Template

Dans le pod RunPod, ouvrir un terminal et tester :

```bash
# V√©rifier nvidia-smi fonctionne
nvidia-smi

# V√©rifier CUDA visible
echo $CUDA_VISIBLE_DEVICES

# V√©rifier PyTorch d√©tecte GPU
python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
```

### 2. Tester Sans Detection GPU

Si le container d√©marre malgr√© l'erreur, v√©rifier que le code fonctionne :

```bash
# Dans le container
python -c "
from app.services.watermark_removal_service import WatermarkRemovalService
service = WatermarkRemovalService(device='cuda')
print('Service initialized:', service.device)
"
```

### 3. V√©rifier Logs RunPod

Regarder les logs d√©taill√©s dans RunPod UI :
- Container startup logs
- Application logs
- GPU detection logs

---

## üìã Checklist de R√©solution

- [ ] Rebuild image avec nouveau Dockerfile (v0.16.1)
- [ ] Push vers Docker registry
- [ ] Update RunPod template avec nouvelle image
- [ ] Tester avec diff√©rents GPU types si √©chec
- [ ] V√©rifier nvidia-smi dans pod terminal
- [ ] V√©rifier PyTorch d√©tecte CUDA
- [ ] Contacter support RunPod si toujours √©chec

---

## üîÑ Rollback si N√©cessaire

Si les optimisations GPU causent trop de probl√®mes, voici comment rollback :

```bash
# Retourner √† la version pr√©c√©dente
docker pull aleou/ffmpeg-worker:0.15.0-serverless

# Ou d√©sactiver GPU dans la config
AI_DEVICE=cpu
```

**Note** : Les optimisations elles-m√™mes sont bonnes, c'est juste RunPod qui a un bug de d√©tection GPU.

---

## üí° Alternative: Tester Localement d'Abord

Pour valider que les optimisations fonctionnent avant de d√©ployer sur RunPod :

```bash
# Build et test local (si GPU disponible)
docker build -t ffmpeg-worker:test --target final_serverless .

docker run --gpus all \
  -e RUNPOD_SKIP_GPU_DETECTION=1 \
  ffmpeg-worker:test \
  python -c "
from app.services.watermark_removal_service import WatermarkRemovalService
import torch
print('CUDA available:', torch.cuda.is_available())
service = WatermarkRemovalService(device='cuda')
print('Service device:', service.device)
"
```

---

## üìû Contact Support

**RunPod Discord** : https://discord.gg/runpod
**GitHub Issues** : https://github.com/runpod/runpod-python/issues

**Mentionner** :
- Erreur: `nvidia-smi: parsing output of line 0: failed to parse (pcie.link.gen.max)`
- Base image: `runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04`
- GPU type utilis√©

---

*Note : Ce n'est PAS un bug de notre code, c'est RunPod infrastructure.*
